[general]

# Do you want YASET to operate in batch mode (i.e. learn multiple models)
batch_mode = false

# If so, how many models do you want to learn?
batch_iter = 10

[data]

# ===================================================================
# DATA FILES
# ===================================================================

# -------------------------------------------------------------------
# TRAIN

# Path to the train file
train_file_path = /path/to/data/train.tab

# -------------------------------------------------------------------
# DEV

# Do you want to specify a dev file ('true' or 'false')?
dev_file_use = true

# If you want to use a dev file, specify the path to file.
# This will be ignored if the value of the parameter 'dev_file_use' is 'false'.
dev_file_path = /path/to/data/dev.tab

# If you do not want to use a dev file, specify the percentage of the training instances that should be kept as
# development instances (float between 0 and 1, e.g. 0.2).
# This will be ignored if the value of the parameter 'dev_file_use' is 'true'.
dev_random_ratio = 0.2

# Do you want to use a random seed for the train/dev split (in the case you do not want to specify a dev file)?
# This will be ignored if the value of the parameter 'dev_file_use' is 'true'.
dev_random_seed_use = true

# If you want to use a random seed, specify the seed value (integer).
# This will be ignored if the value of the parameter 'dev_random_seed_use' is 'false' or if the value of the parameter
# 'dev_file_use' is 'true'.
dev_random_seed_value = 42

# -------------------------------------------------------------------
# DATA PREPROCESSING

# Do you want to lowercase the tokens (first column) before vector looking-up? This will not affect character
# embedding computation (if used).
preproc_lower_input = true

# Do you want to replace digits with "0" in the tokens (first column)?
preproc_replace_digits = true

# -------------------------------------------------------------------
# FEATURES

# Do you want to use other features besides the tokens?
feature_data = false

# If you want to use other features, please specified the column IDs (0-indexed)
feature_columns = 1,2,3

# ===================================================================
# EMBEDDING MODEL
# ===================================================================

# Embedding model type (only 'gensim' or 'word2vec' models are accepted)
embedding_model_type = gensim

# Path to the pretrained embedding model
embedding_model_path = /path/to/data/gensim-model.pkl


# Choose the strategy for Out-Of-Vocabulary (OOV) tokens
# ------------------------------------------------------
#
# 1. 'map': you have a vector for OOV tokens in the embedding file you provided. Set 'embedding_oov_strategy' to 'map'
#   and specify the OOV vector ID ('embedding_oov_map_token_id')
#
# 2. 'replace': following Lample et al. (2016), an OOV vector will be randomly generated and trained by randomly
#   replacing singletons in the training instances with the special token 'unknown'. You can adjust the replacement
#   rate by changing the value of the parameter 'embedding_oov_replace_rate'

embedding_oov_strategy = replace

# This will be ignored if the value of the parameter 'embedding_oov_strategy' is 'replace'.
embedding_oov_map_token_id = #UNK#

# This will be ignored if the value of the parameter 'embedding_oov_strategy' is 'map'.
embedding_oov_replace_rate = 0.5

# ===================================================================
# WORKING DIR
# ===================================================================

# Path where a timestamped working directory will be created
working_dir = /path/to/working/dir/

[training]

# Deep learning model (one choice only: 'bilstm-char-crf')
model_type = bilstm-char-crf

# -------------------------------------------------------------------
# ITERATIONS AND EARLY STOPPING

# Maximum number of iterations for training
max_iterations = 100

# Maximum number of iterations after the best score has been obtained
patience = 10

# -------------------------------------------------------------------
# METRICS

# Metric used for dev performance measurement ('accuracy' or 'conll')
dev_metric = conll

# -------------------------------------------------------------------
# WORD EMBEDDINGS

# Do you want to finetune the word embeddings?
trainable_word_embeddings = true

# -------------------------------------------------------------------
# FEATURE EMBEDDINGS

# Do you want to use features during training?
feature_use = false

# Specify feature embedding size
feature_embedding_size = 10

# -------------------------------------------------------------------
# MISC

# Number of CPU cores to use during training (upper-bound)
cpu_cores = 4

# Mini-bacth size used during training
batch_size = 64

# If using GPU, do you want to store embedding matrices (word and/or character) on GPU memory?
store_matrices_on_gpu = false

# Bucketize input sequences for faster training
bucket_use = false

# -------------------------------------------------------------------
# OPTIMIZATION ALGORITHM

# Optimization algorithm ('adam' or 'sgd')
opt_algo = adam

# Initial learning rate
opt_lr = 0.001

# Use gradient clipping?
opt_gc_use = false
# Specify the gradient clipping type ('clip_by_norm' or 'clip_by_value')
# This will be ignored if the value of the parameter 'opt_gc_use' is 'false'.
opt_gc_type = clip_by_norm
# Gradient clipping value
# This will be ignored if the value of the parameter 'opt_gc_use' is 'false'.
opt_gs_val = 5.0

# Use exponential decay?
opt_decay_use = false
# Exponential decay rate
# This will be ignored if the value of the parameter 'opt_decay_use' is 'false'.
opt_decay_rate = 0.9
# Decay every 'n' iterations
# This will be ignored if the value of the parameter 'opt_decay_use' is 'false'.
opt_decay_iteration = 1

[bilstm-char-crf]

# -------------------------------------------------------------------
# MAIN BiLSTM ARCHITECTURE

# Number of units in the main BiLSTM
hidden_layer_size = 256

# Dropout rate applied during training on input embeddings
dropout_rate = 0.5

# -------------------------------------------------------------------
# CHAR BiLSTM ARCHITECTURE

# Use character embeddings in the model. These embeddings are learned during network training.
use_char_embeddings = true

# Number of units in the character BiLSTM
# This will be ignored if the value of the parameter 'use_char_embeddings' is 'false'.
char_hidden_layer_size = 25

# Character emebdding size
# This will be ignored if the value of the parameter 'use_char_embeddings' is 'false'.
char_embedding_size = 25
